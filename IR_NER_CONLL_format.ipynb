{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_NER_CONLL_format.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mue3aow0No5"
      },
      "source": [
        "paths = ['/content/NER_TWEET_DATA - Ramprasad.csv',\n",
        "         '/content/NER_TWEET_DATA - Pragati.csv',\n",
        "         '/content/NER_TWEET_DATA - Rajeev.csv',\n",
        "         '/content/NER_TWEET_DATA - Ramji.csv',\n",
        "         '/content/NER_TWEET_DATA - Ratnesh.csv'\n",
        "         ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr1lAJVu2G4G"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "import string\n",
        "\n",
        "data = []\n",
        "temp_data = []\n",
        "punk = string.punctuation.replace('-', '').replace('_', '')\n",
        "punk = punk+'â€¦'\n",
        "\n",
        "idx = 0\n",
        "for path in paths:\n",
        "  with open(path, 'r') as csv_file:\n",
        "    csv_data = csv.reader(csv_file)\n",
        "    with open('/content/ner_conll.txt','a+') as write_txt:\n",
        "      for line in csv_data:\n",
        "        cleaned_line = []\n",
        "        cleaned = line[0]\n",
        "        if cleaned.find('http') != -1:\n",
        "          e_idx = 0\n",
        "          for i in range(cleaned.find('http'), len(cleaned)):\n",
        "            e_idx = i\n",
        "            if cleaned[i] == ' ':\n",
        "              cleaned = cleaned[:cleaned.find('http')]+ cleaned[i:]\n",
        "              break\n",
        "          if e_idx == len(cleaned):\n",
        "            cleaned = cleaned[:cleaned.find('http')]\n",
        "\n",
        "        table = cleaned.maketrans(punk, ' '*len(punk))\n",
        "        cleaned = cleaned.translate(table)\n",
        "        cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "        cleaned_line.append(cleaned)\n",
        "        table = line[1].maketrans(punk, ' '*len(punk))\n",
        "        cleaned = line[1].translate(table)\n",
        "        cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "        cleaned_line.append(cleaned)\n",
        "        table = line[2].maketrans(punk, ' '*len(punk))\n",
        "        cleaned = line[2].translate(table)\n",
        "        cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "        cleaned_line.append(cleaned)\n",
        "        write_txt.write(\"'''\")\n",
        "        write_txt.write('\\n')\n",
        "\n",
        "        for word in cleaned_line[0].lower().split():\n",
        "          nouns = list(cleaned_line[1].lower().split())\n",
        "          if word in nouns:\n",
        "            data.append(str(idx)+' '+word+' '+'N')\n",
        "            write_txt.write(str(idx)+' '+word+' '+'N')\n",
        "            write_txt.write('\\n')\n",
        "          else:\n",
        "            data.append(str(idx)+' '+word+' '+'OTH')\n",
        "            write_txt.write(str(idx)+' '+word+' '+'OTH')\n",
        "            write_txt.write('\\n')\n",
        "\n",
        "        write_txt.write(\"'''\")\n",
        "        idx += 1\n",
        "        write_txt.write('\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBHpZRLjfRwr"
      },
      "source": [
        ""
      ]
    }
  ]
}