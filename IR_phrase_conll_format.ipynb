{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_phrase_conll_format.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZyzX0SHrXc4"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "import string"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjFtnJ2-rcOG"
      },
      "source": [
        "data = []\n",
        "temp_data = []\n",
        "noun_phrase = []"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z6HU2Zme5l3",
        "outputId": "dfe0cd66-a5a1-4193-aef9-50859068cdee"
      },
      "source": [
        "!git clone https://ram-asari:akshith.a%401@github.com/ram-asari/IR_ner_phrase-embedding.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IR_ner_phrase-embedding'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 62 (delta 28), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHNtC8I9SwYd"
      },
      "source": [
        "df1 = pd.read_excel('/content/IR_ner_phrase-embedding/NER_TWEET_DATA.xlsx', sheet_name = 'Rajeev', header=None)\n",
        "df2 = pd.read_excel('/content/IR_ner_phrase-embedding/NER_TWEET_DATA.xlsx', sheet_name = 'Ramji', header=None)\n",
        "df3 = pd.read_excel('/content/IR_ner_phrase-embedding/NER_TWEET_DATA.xlsx', sheet_name = 'Ratnesh', header=None)\n",
        "df4 = pd.read_excel('/content/IR_ner_phrase-embedding/NER_TWEET_DATA.xlsx', sheet_name = 'Ramprasad', header=None)\n",
        "df5 = pd.read_excel('/content/IR_ner_phrase-embedding/NER_TWEET_DATA.xlsx', sheet_name = 'Pragati', header=None)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hy6eOGefMvU"
      },
      "source": [
        "# df = pd.read_csv('/content/IR_ner_phrase-embedding/NER_tweet_data.csv', header= None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkLB1mu9UKYc"
      },
      "source": [
        "df = df1.append(df2)\n",
        "df = df.append(df3)\n",
        "df = df.append(df4)\n",
        "df = df.append(df5)\n",
        "df = df.sample(frac = 1) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "lY9NDAJzmg-l",
        "outputId": "fcd206eb-545f-4541-9bdd-d91bd6e96d54"
      },
      "source": [
        "\n",
        "df.dropna(inplace=True)\n",
        "df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>YE #Notebandi se aam Kisan &amp; Public ko line me...</td>\n",
              "      <td>Public:logo</td>\n",
              "      <td>line me laga k tabah kiya: logo ko: tang kar RAHE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>#GST k bad to aisa lag rha he ki restaurant me...</td>\n",
              "      <td>narendramodi:GST:PMOIndia</td>\n",
              "      <td>aisa kya urgent: salaah kar lete</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Chai bechne wala aaj pure desh ko chai pila ra...</td>\n",
              "      <td>desh:MODI:SARKAR!:GST</td>\n",
              "      <td>Chai bechne wala:ab ki baar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Humne sarkar se kaha tha #GST mein itne slabs ...</td>\n",
              "      <td>RahulGandhi:Ahmedabad</td>\n",
              "      <td>ka limit hona</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>Sare tax khatam karke #GST lagu kar diya...bas...</td>\n",
              "      <td>narendramodi:GST</td>\n",
              "      <td>Sare tax khatam: ab sari jatiyan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>Barish ko bhi pata hai kal #GST bharne ki aakh...</td>\n",
              "      <td>GST:Barish</td>\n",
              "      <td>ko bhi pata hai  :  is liye overtime chal raha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>three months me #GST aa rha hai..Tab jamke bad...</td>\n",
              "      <td>GST:logo</td>\n",
              "      <td>aa rha hai:  kyu naraj Karenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>Jaago....govts. jaago....ya citizens hi hathiy...</td>\n",
              "      <td>NIRBHAYA:India</td>\n",
              "      <td>aap logo se: kuch ho nahi: hadd hai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Muslim personal law board ko khatam kiya jaye ...</td>\n",
              "      <td>Muslim:india</td>\n",
              "      <td>khatam kiya jaye  :</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>\"Inka kaam hi hai, inhe paise hi is kaam ke mi...</td>\n",
              "      <td>nation</td>\n",
              "      <td>kaam hi hai: inhe paise hi is kaam: milte hai:...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>896 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0  ...                                                  2\n",
              "92   YE #Notebandi se aam Kisan & Public ko line me...  ...  line me laga k tabah kiya: logo ko: tang kar RAHE\n",
              "195  #GST k bad to aisa lag rha he ki restaurant me...  ...                   aisa kya urgent: salaah kar lete\n",
              "0    Chai bechne wala aaj pure desh ko chai pila ra...  ...                        Chai bechne wala:ab ki baar\n",
              "6    Humne sarkar se kaha tha #GST mein itne slabs ...  ...                                      ka limit hona\n",
              "168  Sare tax khatam karke #GST lagu kar diya...bas...  ...                   Sare tax khatam: ab sari jatiyan\n",
              "..                                                 ...  ...                                                ...\n",
              "71   Barish ko bhi pata hai kal #GST bharne ki aakh...  ...  ko bhi pata hai  :  is liye overtime chal raha...\n",
              "147  three months me #GST aa rha hai..Tab jamke bad...  ...                     aa rha hai:  kyu naraj Karenge\n",
              "169  Jaago....govts. jaago....ya citizens hi hathiy...  ...                aap logo se: kuch ho nahi: hadd hai\n",
              "47   Muslim personal law board ko khatam kiya jaye ...  ...                               khatam kiya jaye  : \n",
              "71   \"Inka kaam hi hai, inhe paise hi is kaam ke mi...  ...  kaam hi hai: inhe paise hi is kaam: milte hai:...\n",
              "\n",
              "[896 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yETTfcDsuK0l"
      },
      "source": [
        "df = df[df[2] != 0]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37LHxUdsohmN"
      },
      "source": [
        "df_list = df.values.tolist()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WShsKiA8o0n9",
        "outputId": "9237d108-9cdc-4f65-8b44-8c48eaa7ff36"
      },
      "source": [
        "df_list[5][2].split(':')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abhi_haal hi_m', ' fake rajput']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Gzh1Y60B9D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCFutBK-DmnU"
      },
      "source": [
        "# Phrases dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwNBK-oCvkt6"
      },
      "source": [
        "punk = string.punctuation.replace('-', '').replace('_', '')\n",
        "punk = punk+'…'\n",
        "phrase_dict = {}\n",
        "ph_count = 0\n",
        "\n",
        "for line in df_list:\n",
        "\n",
        "  for phrase in line[2].split(':'):\n",
        "    table = phrase.maketrans(punk, ' '*len(punk))\n",
        "    cleaned = phrase.translate(table)\n",
        "    cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.lower()\n",
        "    if len(cleaned) > 1:\n",
        "      if phrase_dict.get(cleaned, -1) == -1:\n",
        "        phrase_dict[cleaned] = 1\n",
        "      else:\n",
        "        phrase_dict[cleaned] += 1\n",
        "    \n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5krtj0H6EtES"
      },
      "source": [
        "phrase_count = 0 \n",
        "for ph, count in phrase_dict.items():\n",
        "  if count > 5 :\n",
        "    print(ph)\n",
        "    phrase_count += 1\n",
        "print(phrase_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm0BnNhVDqUR"
      },
      "source": [
        "# CONLL formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8INV_zQB0d7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f5148e-7096-4ed5-a933-bd617668e9a9"
      },
      "source": [
        "# tot_data = [train_data, val_data]\n",
        "punk = string.punctuation.replace('-', '').replace('_', '')\n",
        "punk = punk+'…'\n",
        "ccc = 0\n",
        "\n",
        "# for iiii, gen_data in enumerate(tot_data):\n",
        "  # data = []\n",
        "with open('/content/ner_conll.txt','w') as write_txt:\n",
        "  for idx, line in enumerate(df_list):\n",
        "    cleaned_line = []\n",
        "    cleaned = line[0]\n",
        "    if cleaned.find('http') != -1:\n",
        "      end_idx = 0\n",
        "      for i in range(cleaned.find('http'), len(cleaned)):\n",
        "        end_idx = i\n",
        "        if cleaned[i] == ' ':\n",
        "          cleaned = cleaned[:cleaned.find('http')]+ cleaned[i:]\n",
        "          break\n",
        "      if end_idx == len(cleaned):\n",
        "        cleaned = cleaned[:cleaned.find('http')]\n",
        "\n",
        "    table = cleaned.maketrans(punk, ' '*len(punk))\n",
        "    cleaned = cleaned.translate(table)\n",
        "    cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.lower()\n",
        "    # data.append(cleaned)\n",
        "    cleaned_line.append(cleaned)\n",
        "\n",
        "\n",
        "    table = line[1].maketrans(punk, ' '*len(punk))\n",
        "    cleaned = line[1].translate(table)\n",
        "    cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "    cleaned = cleaned.lower()\n",
        "    cleaned_line.append(cleaned)\n",
        "\n",
        "    table = line[2].maketrans(punk, ' '*len(punk))\n",
        "    cleaned = line[2].translate(table)\n",
        "    cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "    cleaned = cleaned.lower()\n",
        "    cleaned_line.append(cleaned)\n",
        "\n",
        "    #this is for phrase replacing\n",
        "    ##############################################\n",
        "    for i_id, phrase  in enumerate(phrase_dict.items()):\n",
        "      # print(phrase[0], i_id)\n",
        "      cleaned_line[0] = cleaned_line[0].replace(phrase[0], 's*s*r^'+str(i_id)) \n",
        "      # print(cleaned_line[0])\n",
        "    ############################################\n",
        "\n",
        "    write_txt.write(\"'''\")\n",
        "    write_txt.write('\\n')\n",
        "\n",
        "    for word in cleaned_line[0].lower().split():\n",
        "      nouns = list(cleaned_line[1].lower().split())\n",
        "      if word in nouns:\n",
        "        # data.append(str(idx)+' '+word+' '+'N')\n",
        "        write_txt.write(str(idx)+' '+word+' '+'N')\n",
        "        ccc+= 1\n",
        "        write_txt.write('\\n')\n",
        "      else:\n",
        "        # data.append(str(idx)+' '+word+' '+'OTH')\n",
        "        write_txt.write(str(idx)+' '+word+' '+'OTH')\n",
        "        write_txt.write('\\n')\n",
        "\n",
        "    write_txt.write(\"'''\")\n",
        "    idx += 1\n",
        "    write_txt.write('\\n')\n",
        "    write_txt.write('\\n')\n",
        "\n",
        "  print(ccc)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC_8_ur0eOCB"
      },
      "source": [
        "# Creating training and validation split for Pharase embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bapHLcZuaRGa",
        "outputId": "36c6ab77-b9a2-4204-bdc6-f8b8fb6ee032"
      },
      "source": [
        "train_data  = df_list[:627] \n",
        "val_data = df_list[627:]\n",
        "tot_data = [train_data, val_data]\n",
        "punk = string.punctuation.replace('-', '').replace('_', '')\n",
        "punk = punk+'…'\n",
        "ccc = 0\n",
        "\n",
        "for iiii, gen_data in enumerate(tot_data):\n",
        "  data = []\n",
        "  with open('/content/ner_conll_phrase_'+str(iiii)+'.txt','w') as write_txt:\n",
        "    for idx, line in enumerate(df_list):\n",
        "      cleaned_line = []\n",
        "      cleaned = line[0]\n",
        "      if cleaned.find('http') != -1:\n",
        "        end_idx = 0\n",
        "        for i in range(cleaned.find('http'), len(cleaned)):\n",
        "          end_idx = i\n",
        "          if cleaned[i] == ' ':\n",
        "            cleaned = cleaned[:cleaned.find('http')]+ cleaned[i:]\n",
        "            break\n",
        "        if end_idx == len(cleaned):\n",
        "          cleaned = cleaned[:cleaned.find('http')]\n",
        "\n",
        "      table = cleaned.maketrans(punk, ' '*len(punk))\n",
        "      cleaned = cleaned.translate(table)\n",
        "      cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "      cleaned = cleaned.strip()\n",
        "      cleaned = cleaned.lower()\n",
        "      # data.append(cleaned)\n",
        "      cleaned_line.append(cleaned)\n",
        "\n",
        "\n",
        "      table = line[1].maketrans(punk, ' '*len(punk))\n",
        "      cleaned = line[1].translate(table)\n",
        "      cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "      cleaned = cleaned.lower()\n",
        "      cleaned_line.append(cleaned)\n",
        "\n",
        "      table = line[2].maketrans(punk, ' '*len(punk))\n",
        "      cleaned = line[2].translate(table)\n",
        "      cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "      cleaned = cleaned.lower()\n",
        "      cleaned_line.append(cleaned)\n",
        "\n",
        "      #this is for phrase replacing\n",
        "      ##############################################\n",
        "      for i_id, phrase  in enumerate(phrase_dict.items()):\n",
        "        # print(phrase[0], i_id)\n",
        "        cleaned_line[0] = cleaned_line[0].replace(phrase[0], 's*s*r^'+str(i_id)) \n",
        "        # print(cleaned_line[0])\n",
        "      ############################################\n",
        "\n",
        "      # write_txt.write(\"'''\")\n",
        "      # write_txt.write('\\n')\n",
        "\n",
        "      for word in cleaned_line[0].lower().split():\n",
        "        nouns = list(cleaned_line[1].lower().split())\n",
        "        if word in nouns:\n",
        "          write_txt.write(word+' '+'N')\n",
        "          ccc+= 1\n",
        "          write_txt.write('\\n')\n",
        "        else:\n",
        "          write_txt.write(word+' '+'OTH')\n",
        "          write_txt.write('\\n')\n",
        "\n",
        "\n",
        "      idx += 1\n",
        "      write_txt.write('\\n')\n",
        "\n",
        "    print('number of nouns ',ccc)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of nouns  2234\n",
            "number of nouns  4468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLH0mqYNeqKa"
      },
      "source": [
        "# creating training and validation split for word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91rwGQAWeg0H",
        "outputId": "1211c25e-653a-4812-9489-85bee9c5917c"
      },
      "source": [
        "train_data  = df_list[:627] \n",
        "val_data = df_list[627:]\n",
        "tot_data = [train_data, val_data]\n",
        "\n",
        "ccc = 0\n",
        "\n",
        "for iiii, gen_data in enumerate(tot_data):\n",
        "  data = []\n",
        "  with open('/content/ner_conll_'+str(iiii)+'.txt','w') as write_txt:\n",
        "    for idx, line in enumerate(df_list):\n",
        "      cleaned_line = []\n",
        "      cleaned = line[0]\n",
        "      if cleaned.find('http') != -1:\n",
        "        end_idx = 0\n",
        "        for i in range(cleaned.find('http'), len(cleaned)):\n",
        "          end_idx = i\n",
        "          if cleaned[i] == ' ':\n",
        "            cleaned = cleaned[:cleaned.find('http')]+ cleaned[i:]\n",
        "            break\n",
        "        if end_idx == len(cleaned):\n",
        "          cleaned = cleaned[:cleaned.find('http')]\n",
        "\n",
        "      table = cleaned.maketrans(punk, ' '*len(punk))\n",
        "      cleaned = cleaned.translate(table)\n",
        "      cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "      cleaned = cleaned.strip()\n",
        "      cleaned = cleaned.lower()\n",
        "      # data.append(cleaned)\n",
        "      cleaned_line.append(cleaned)\n",
        "\n",
        "\n",
        "      table = line[1].maketrans(punk, ' '*len(punk))\n",
        "      cleaned = line[1].translate(table)\n",
        "      cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "      cleaned = cleaned.lower()\n",
        "      cleaned_line.append(cleaned)\n",
        "\n",
        "      table = line[2].maketrans(punk, ' '*len(punk))\n",
        "      cleaned = line[2].translate(table)\n",
        "      cleaned = re.sub(r\"\\s+\",\" \",cleaned)\n",
        "      cleaned = cleaned.lower()\n",
        "      cleaned_line.append(cleaned)\n",
        "\n",
        "      #this is for phrase replacing\n",
        "      ##############################################\n",
        "      # for i_id, phrase  in enumerate(phrase_dict.items()):\n",
        "      #   # print(phrase[0], i_id)\n",
        "      #   cleaned_line[0] = cleaned_line[0].replace(phrase[0], 's*s*r^'+str(i_id)) \n",
        "      #   # print(cleaned_line[0])\n",
        "      ############################################\n",
        "\n",
        "      # write_txt.write(\"'''\")\n",
        "      # write_txt.write('\\n')\n",
        "\n",
        "      for word in cleaned_line[0].lower().split():\n",
        "        nouns = list(cleaned_line[1].lower().split())\n",
        "        if word in nouns:\n",
        "          write_txt.write(word+' '+'N')\n",
        "          ccc+= 1\n",
        "          write_txt.write('\\n')\n",
        "        else:\n",
        "          write_txt.write(word+' '+'OTH')\n",
        "          write_txt.write('\\n')\n",
        "\n",
        "\n",
        "      idx += 1\n",
        "      write_txt.write('\\n')\n",
        "\n",
        "    print('number of nouns ',ccc)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of nouns  2411\n",
            "number of nouns  4822\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}